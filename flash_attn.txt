INFO 07-15 08:07:30 [__init__.py:253] Automatically detected platform cuda.
Model: /scratch/usr/quantized_model/
Number of prompts: 25
Batch size: 1
Iterations: 1
Warmup iterations: 0
Effective kv_cache_dtype: fp8
INFO 07-15 08:07:38 [config.py:852] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 07-15 08:07:38 [config.py:852] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.
WARNING 07-15 08:07:40 [arg_utils.py:1786] Speculative Decoding is not supported by the V1 Engine. Falling back to V0. 
WARNING 07-15 08:07:40 [arg_utils.py:1594] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.
INFO 07-15 08:07:40 [config.py:1637] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor
WARNING 07-15 08:07:41 [modelopt.py:49] Detected ModelOpt fp8 checkpoint. Please note that the format is experimental and could change.
INFO 07-15 08:07:41 [llm_engine.py:230] Initializing a V0 LLM engine (v0.1.dev7506+ge5d398a) with config: model='/scratch/usr/quantized_model/', speculative_config=SpeculativeConfig(method='draft_model', model='/scratch/usr/mistral-small-fp8', num_spec_tokens=5), tokenizer='/scratch/usr/quantized_model/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt, enforce_eager=False, kv_cache_dtype=fp8,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/scratch/usr/quantized_model/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":1,"local_cache_dir":null}, use_cached_outputs=False, 
WARNING 07-15 08:07:41 [__init__.py:2869] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
WARNING 07-15 08:07:41 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
Traceback (most recent call last):
  File "/home/jdebache/vllm/vllm_sample.py", line 219, in <module>
    main()
  File "/home/jdebache/vllm/vllm_sample.py", line 117, in main
    llm = LLM(
          ^^^^
  File "/home/jdebache/vllm/vllm/entrypoints/llm.py", line 274, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/engine/llm_engine.py", line 501, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/engine/llm_engine.py", line 477, in from_vllm_config
    return cls(
           ^^^^
  File "/home/jdebache/vllm/vllm/engine/llm_engine.py", line 265, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/executor/executor_base.py", line 287, in __init__
    super().__init__(*args, **kwargs)
  File "/home/jdebache/vllm/vllm/executor/executor_base.py", line 53, in __init__
    self._init_executor()
  File "/home/jdebache/vllm/vllm/executor/mp_distributed_executor.py", line 124, in _init_executor
    self._run_workers("init_worker", all_kwargs)
  File "/home/jdebache/vllm/vllm/executor/mp_distributed_executor.py", line 186, in _run_workers
    driver_worker_output = run_method(self.driver_worker, sent_method,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/utils/__init__.py", line 2943, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/worker/worker_base.py", line 595, in init_worker
    self.worker = worker_class(**kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/spec_decode/spec_decode_worker.py", line 76, in create_spec_worker
    target_worker = cls(*args, **kwargs)
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/worker/worker.py", line 87, in __init__
    self.model_runner: GPUModelRunnerBase = ModelRunnerClass(
                                            ^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/worker/model_runner.py", line 1125, in __init__
    self.attn_backend = get_attn_backend(
                        ^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/attention/selector.py", line 153, in get_attn_backend
    return _cached_get_attn_backend(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/attention/selector.py", line 193, in _cached_get_attn_backend
    attention_cls = current_platform.get_attn_backend_cls(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jdebache/vllm/vllm/platforms/cuda.py", line 332, in get_attn_backend_cls
    raise ValueError(
ValueError: Invalid attention backend for cuda, with use_v1: False use_mla: False
ERROR 07-15 08:07:41 [multiproc_worker_utils.py:121] Worker VllmWorkerProcess pid 3892383 died, exit code: -15
INFO 07-15 08:07:41 [multiproc_worker_utils.py:125] Killing local vLLM worker processes
